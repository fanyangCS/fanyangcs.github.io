<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://fanyangcs.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://fanyangcs.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-09-28T15:05:01+00:00</updated><id>https://fanyangcs.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">微软亚洲研究院多项创新技术，弥合大模型低比特量化与终端部署间鸿沟</title><link href="https://fanyangcs.github.io/blog/2024/microsoft-research-asia-blog-chinese-low-bit-quantization/" rel="alternate" type="text/html" title="微软亚洲研究院多项创新技术，弥合大模型低比特量化与终端部署间鸿沟"/><published>2024-08-21T00:00:00+00:00</published><updated>2024-08-21T00:00:00+00:00</updated><id>https://fanyangcs.github.io/blog/2024/microsoft-research-asia-blog-chinese-low-bit-quantization</id><content type="html" xml:base="https://fanyangcs.github.io/blog/2024/microsoft-research-asia-blog-chinese-low-bit-quantization/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[编者按：在人工智能领域，模型参数的增多往往意味着性能的提升。但随着模型规模的扩大，其对终端设备的算力与内存需求 […]]]></summary></entry><entry><title type="html">两个小模型互相验证，直接比肩大模型？微软的rStar甚至没用CoT和微调 | 机器之心</title><link href="https://fanyangcs.github.io/blog/2024/rstarcot/" rel="alternate" type="text/html" title="两个小模型互相验证，直接比肩大模型？微软的rStar甚至没用CoT和微调 | 机器之心"/><published>2024-08-16T00:00:00+00:00</published><updated>2024-08-16T00:00:00+00:00</updated><id>https://fanyangcs.github.io/blog/2024/rstarcot--</id><content type="html" xml:base="https://fanyangcs.github.io/blog/2024/rstarcot/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[互相检查，让小模型也能解决大问题。]]></summary></entry><entry><title type="html">To prospective interns</title><link href="https://fanyangcs.github.io/blog/2024/intern/" rel="alternate" type="text/html" title="To prospective interns"/><published>2024-08-01T12:00:00+00:00</published><updated>2024-08-01T12:00:00+00:00</updated><id>https://fanyangcs.github.io/blog/2024/intern</id><content type="html" xml:base="https://fanyangcs.github.io/blog/2024/intern/"><![CDATA[<p>If you consider yourself a good systems guy and looking for an internship at MSR-Asia, please drop me an email. I also recruit students taking gap year.</p> <p>If you are interested in systems research and don’t know where to begin with, you could start by reading the papers mentioned <a href="https://www.sigops.org/awards/hof/">here</a>.</p> <p>If you are looking for advice on your research, Richard Hamming’s talk on “<a href="/assets/pdf/YouAndYourResearch.pdf">You and Your Research</a>” is a must-read.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[If you consider yourself a good systems guy and looking for an internship at MSR-Asia, please drop me an email. I also recruit students taking gap year.]]></summary></entry><entry><title type="html">大语言模型应用如何实现端到端优化？</title><link href="https://fanyangcs.github.io/blog/2024/microsoft-research-asia-blog-chinese-parrot/" rel="alternate" type="text/html" title="大语言模型应用如何实现端到端优化？"/><published>2024-07-25T00:00:00+00:00</published><updated>2024-07-25T00:00:00+00:00</updated><id>https://fanyangcs.github.io/blog/2024/microsoft-research-asia-blog-chinese-parrot</id><content type="html" xml:base="https://fanyangcs.github.io/blog/2024/microsoft-research-asia-blog-chinese-parrot/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[编者按：基于大语言模型（LLMs）开发的应用目前主要使用公共 LLMs 服务提供的 API 进行，但是这些 L […]]]></summary></entry><entry><title type="html">nnScaler：重塑深度学习并行策略，大幅提升训练效率</title><link href="https://fanyangcs.github.io/blog/2024/nnscaler/" rel="alternate" type="text/html" title="nnScaler：重塑深度学习并行策略，大幅提升训练效率"/><published>2024-07-22T00:00:00+00:00</published><updated>2024-07-22T00:00:00+00:00</updated><id>https://fanyangcs.github.io/blog/2024/nnscaler</id><content type="html" xml:base="https://fanyangcs.github.io/blog/2024/nnscaler/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[编者按：深度学习技术已经在图像识别、语音识别、自然语言处理、搜索推荐等多个领域不断展现出巨大的应用价值。然而， […]]]></summary></entry><entry><title type="html">Unified Database: Laying the foundation for large language model vertical applications - Microsoft Research</title><link href="https://fanyangcs.github.io/blog/2024/unified-database-laying-the-foundation-for-large-language-model-vertical-applications-microsoft-research/" rel="alternate" type="text/html" title="Unified Database: Laying the foundation for large language model vertical applications - Microsoft Research"/><published>2024-07-10T00:00:00+00:00</published><updated>2024-07-10T00:00:00+00:00</updated><id>https://fanyangcs.github.io/blog/2024/unified-database-laying-the-foundation-for-large-language-model-vertical-applications---microsoft-research</id><content type="html" xml:base="https://fanyangcs.github.io/blog/2024/unified-database-laying-the-foundation-for-large-language-model-vertical-applications-microsoft-research/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">On annual performance review</title><link href="https://fanyangcs.github.io/blog/2024/PerformanceReview/" rel="alternate" type="text/html" title="On annual performance review"/><published>2024-07-01T12:00:00+00:00</published><updated>2024-07-01T12:00:00+00:00</updated><id>https://fanyangcs.github.io/blog/2024/PerformanceReview</id><content type="html" xml:base="https://fanyangcs.github.io/blog/2024/PerformanceReview/"><![CDATA[<p>I find this <a href="https://www.norvig.com/performance-review.html">annual performance review</a> amusing. It happened in ‘05. Not 2005, but 1905.</p> <p>In the year of 1905, the person being reviewed was given an overall rating of 3 out of 5, i.e., an average score. His manager commented that the person devoted his time to “publishing a series of outside papers” and “he has done reasonably well”.</p> <p>As a matter of fact, in 1905, the person published 5 papers. One of them, “On a heuristic viewpoint concerning the production and transformation of light”, eventually led to the Nobel Prize in Physics in 1921. And this wasn’t even his best result that year – the other being his theory on Relativity.</p> <p>From his manager’s perspective, this was an average year for Albert Einstein. For Physics and Human Being, however, this was a revolutionary year.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[I find this annual performance review amusing. It happened in ‘05. Not 2005, but 1905.]]></summary></entry><entry><title type="html">统一化数据库：为大语言模型垂域应用奠定基础</title><link href="https://fanyangcs.github.io/blog/2024/microsoft-research-asia-blog-chinese-unified-database/" rel="alternate" type="text/html" title="统一化数据库：为大语言模型垂域应用奠定基础"/><published>2024-04-24T00:00:00+00:00</published><updated>2024-04-24T00:00:00+00:00</updated><id>https://fanyangcs.github.io/blog/2024/microsoft-research-asia-blog-chinese-unified-database</id><content type="html" xml:base="https://fanyangcs.github.io/blog/2024/microsoft-research-asia-blog-chinese-unified-database/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[编者按：检索增强生成（RAG）技术因在减少生成幻觉和虚构信息方面的显著效果，以及对知识及时更新能力的改善，正逐 […]]]></summary></entry><entry><title type="html">LongRoPE：超越极限，将大模型上下文窗口扩展超过200万tokens</title><link href="https://fanyangcs.github.io/blog/2024/longrope200tokens/" rel="alternate" type="text/html" title="LongRoPE：超越极限，将大模型上下文窗口扩展超过200万tokens"/><published>2024-04-16T00:00:00+00:00</published><updated>2024-04-16T00:00:00+00:00</updated><id>https://fanyangcs.github.io/blog/2024/longrope200tokens</id><content type="html" xml:base="https://fanyangcs.github.io/blog/2024/longrope200tokens/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[编者按：大模型的飞速发展给人们的生活带来了前所未有的便利。我们是否能够设想利用大模型的潜力，快速扫描整部百科全 […]]]></summary></entry><entry><title type="html">科研上新 | 大模型推进科研边界；大模型的道德价值对齐；优化动态稀疏深度学习模型；十亿规模向量搜索的高效更新</title><link href="https://fanyangcs.github.io/blog/2023/microsoft-research-asia-blog-chinese-new-arrival-in-research-3/" rel="alternate" type="text/html" title="科研上新 | 大模型推进科研边界；大模型的道德价值对齐；优化动态稀疏深度学习模型；十亿规模向量搜索的高效更新"/><published>2023-10-31T00:00:00+00:00</published><updated>2023-10-31T00:00:00+00:00</updated><id>https://fanyangcs.github.io/blog/2023/microsoft-research-asia-blog-chinese-new-arrival-in-research-3</id><content type="html" xml:base="https://fanyangcs.github.io/blog/2023/microsoft-research-asia-blog-chinese-new-arrival-in-research-3/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[编者按：欢迎阅读“科研上新”栏目！“科研上新”汇聚了微软亚洲研究院最新的创新成果与科研动态。在这里，你可以快速 […]]]></summary></entry></feed>