---
layout: post
title: A podcast on AI reasoning
date: 2025-12-05 12:00:00
description: How to fill the seemingly insurmountable gap between “correlation” and “causality”?
tags: 
categories: 
featured: false
---

Recently, I was on a [podcast](https://fanyangcs.github.io/news/AIReasoningPodcast/) discussing AI reasoning. Below is the transcript of the podcast. It is in Chinese. I will see if I get a chance to have an English version.

<hr>

#### **背景：**

<em>
一提到“推理”，很多人第一反应可能是侦探小说里的福尔摩斯：通过蛛丝马迹一步步找出真相。人类的推理确实是这样，我们会依靠逻辑、常识和经验，从已知条件出发，逐步得出结论。 

那 AI 的推理呢？它当然不会去破案，但它要面对的，其实是类似的过程：从已有信息里，分析出合理的结果，甚至能够延伸出新的可能性。比如，你对一个智能助手说：“今天下雨了，我要出门”，它如果只回答“带伞”，这其实还不算推理；但如果它能继续想到“可能要穿防水外套”，“雨天路滑可以考虑公共交通”，这就是在做更接近人类的推理。

但是，AI的推理能力也不是一开始就这么强的。在AI发展的历史中，我们通常把AI推理分为两个主要阵营：

第一个是“符号推理”（Symbolic Reasoning）。这是比较传统的方法。你可以想象一下，它就是给AI设定了一堆严谨的逻辑规则，比如“如果 A 成立，并且 B 成立，那么 C 就成立”。它的优点是严谨、可解释，你可以清楚地知道AI是怎么得出结论的。但它的缺点也很明显：现实世界太复杂了，很难穷尽所有规则，而且面对不确定的、模糊的问题时，它就束手无策了。

第二个是“非符号推理”（Non-symbolic Reasoning），这主要基于数据进行学习，通过神经网络发现数据中的隐藏模式。像我们现在经常听到的大语言模型，它们的推理能力就属于这一类。它不像符号推理那样有一条条清晰的逻辑链，更像是一种“直觉”。它在海量数据中看到了无数次“下雨”和“带伞”的关联，所以当你说“下雨”时，它就能“直觉”地给出“带伞”这个答案。它的优点是擅长处理复杂、开放的问题，但有时也会犯一些低级错误，甚至给出看似合理但实际上是错误的回答，也就是我们常说的“幻觉”。

这两种方法各有优势和短板。而现在最前沿的研究，就是如何把它们结合起来，让AI既能有强大的“直觉”，又能进行严密的“逻辑”思考。

带着这个背景，我们就更容易理解后面要聊的内容了。好了，话不多说，让我们进入今天的正题吧！
</em>

#### **对话环节：**
##### **主持人**

杨凡博士你好，现在我们先从一个基础的问题聊起。很多听众对“推理”的理解可能还停留在人类的逻辑推理层面，比如福尔摩斯那样的逻辑推理。那么在人工智能的语境下，我们说的“AI推理”和传统意义上的逻辑推理，到底有什么本质区别呢？能不能理解为，AI推理更多是基于大数据的概率性归纳？

###### **我**

这个问题非常有意思。其实从某种意义上来说，AI推理和逻辑推理就像人工智能和人类智能的区别一样。人工智能研究的目标，就是希望做到接近人类智能的水平。而推理能力正是智能最本质的体现之一，所以这也是我们对智能特别感兴趣的一个原因。

根据维基百科的介绍，推理就是从大量信息中得出结论的能力。如果得出的结论是令人信服的，那它就需要一定的逻辑。换句话说，为了保证推理的正确性，就需要遵循一些比较严谨的过程，这就是逻辑。从抽象层面上说，它可以用数学来表达。

很多人会觉得大语言模型的推理是基于概率统计的大数据归纳。这牵涉到统计方面的一个本质问题：从大数据中归纳出来的现象，究竟意味着什么？是纯粹的相关性，还是因果性？如果只是相关性，那AI推理就不应该有因果性，可推理本身就是基于因果性的。

如果AI 是基于大数据的归纳，它只是浮现出相关性，那么就不应该展现出强大的推理能力。现在的AI推理确实经常犯错，但它在过去两年的发展中，推理正确性也越来越高。而且以人类的知识来检验的话，它这中间展现出的因果性是相当惊人的。为什么会展现出这样的一种特性，以及这种特性能走多远？

现在学界对这个问题并没有统一答案。我个人更倾向认为，在基于概率的模型上，我们仍然有机会把推理做到一个很高的水准，甚至达到实用层面。比如说，在初高中层面的逻辑和数学题上，大模型如果展现出相当可靠的推理能力，那么就有可能在实际应用里发挥价值。

至于AI推理和传统意义上的人类推理的区别，我觉得目前还没有明确答案。就像人工智能和人类智能之间的差异一样，我们只知道人脑的生物学结构和大模型的构造完全不一样。但这并不意味着它们不能共享某些特性。

我更愿意把智能看成是一件很“纯粹的事”， 智能就是智能，你可以用方法A来实现智能，也可以用方法B来实现。人类和大模型就是实现智能的两种不同方式，其中可能存在相关性。这是我现在的一个看法。

##### **主持人**

你提到推理的正确性，这让我想到另一个问题。很多人觉得“推理就是让大模型更聪明”。可在学术界和工业界，大家又特别关注计算速度。那么 ，你怎么看推理速度和智能之间的关系？

##### **我**

我觉得只有正确的推理才能算聪明，尤其是面对复杂问题的时候，能得出正确答案才是真正的智能。人和 AI 都会在速度和质量之间做平衡。对于一些问题，我们都希望得到对的答案。但想得快却经常出错，其实没有意义。大语言模型也是一样，光快不够，必须尽量保证推理正确。

当然，速度在推理中也非常重要。很多复杂问题可能需要成千上万步的推理步骤，如果每一步都很慢，那最终问题也解不出来。而且人也不会等这么长时间，大多数情况下，我们还是希望答案尽快出来。

另外，如果在一些相对简单的问题上都思考得很慢，那别人也很难相信你在复杂问题上即使坚持长时间能给出结果。我们通常觉得一个人聪明表现为思考非常快，当然回答也是对的。

所以从智能的角度来说，速度和质量之间确实有一个微妙的平衡，不能做极端的选择。

##### **主持人**

非常清晰的观点。所以在推理能力上，既要准确，也要兼顾速度，两者确实需要平衡。那接下来聊一聊你团队的研究方向。你的团队最近在探索新的推理方式，能不能给我们介绍一下研究内容？这些方法和传统的深度学习推理相比，最大的突破点在哪里？在实验过程中有没有什么让你们出乎意料的发现？

##### **我**

我们现在在探讨的一个推理范式叫做 neural symbolic reasoning。Neural 就是神经，symbolic 就是符号主义的推导。其实这个范式并不算新，人工智能早期就有符号主义和连接主义的争论。在很长一段时间里，符号主义占优势，因为连接主义没有展现出让人兴奋的结果。某种意义上，今天的计算科学很多知识都还是属于符号主义的范畴。

随着大语言模型的兴起，连接主义在一些传统计算机难以解决的问题上展现出惊人的实力，比如语音识别、图像识别和自然语言理解等等。这是传统计算机做不到的。于是现在大家普遍认为，连接主义，也就是大语言模型，可能是走向 AI最重要的技术路径。

在这样的范式下，研究社区都在追求所谓的 scaling law，就是追求模型规模、训练数据规模和计算规模，把这三者进一步扩展，智能表现就会不断提升。去年下半年，大家又注意到了时间因素，我称之为scaling law的第四维度。这被称为 test-time scaling，也就是说，模型思考的时间越长，它的智能水平往往越高。

我们团队在一两年前就开始关注这种推理范式。当时有两个很朴素的想法。第一，scaling law 本身很符合人类直觉，比如人的大脑容量比其他动物大，神经元数量更多，这完全符合scaling law，所以我们的智能表现就比别的更好了，对吧？

第二个维度是数据，然后就是计算力。你要思考的时候，你需要的这个脑力是很高的。这个脑力就相当于计算力。我们那个时候就觉得人类进一步提高，那不就是要思考的久一点吗？所以，当时这个范式还没有出现的时候，我们就认为我们肯定是要在脑子中不断思考。我们当时把这种范式称之为 self-play，也就是在脑中不断演练思考。我们去年的成果第一次用公开的方法告诉大家self-play，或者说所谓的test time scaling是怎么样提升智能的。这也是我们觉得探索智能进步的路径可以参考人类的一个原因。

我们还思考neural symbolic reasoning。这是什么意思呢？我们认为从符号主义和连接主义的角度来讲，他们可能是一种互补关系。那时我们看到大语言模型推理肯定有很多很多问题，对吧？毕竟两年前大语言模型推理水平还停留在小学阶段，现在已经可以把奥林匹克的数学竞赛的金牌都给赢下来。我们当时也并不是认为大语言模型做不到，只是它也像人一样需要一个“脚手架”。 也就是用符号化的工具来帮助大语言模型在推理的时候做一种验证，所谓的verification，有了这个verification，那么就能极大地提高推理的可靠性。

人类也会犯错，但我们可以通过复核、老师批改、甚至借助工具来验证，对吧？比如人类经常用的一些工具，比如数学的工具，像计算器。对 AI 来说，我们大量使用了自动定理证明来做验证。我们目前来讲的话，看到的这个范式也是起到了一个作用。

事实上，研究社区里现在也在讨论类似的方向，虽然名字可能不同，比如有人叫 self-verifiable learning，但本质是一样的。推理在数学领域是相对比较容易被验证的，所以你会看到现在前沿的大模型都在打数学竞赛、编程比赛。这也是因为这些领域可以提供清晰的验证标准。

所以我认为我们过去探索的这些方向算不上新的范式，而是沿着智能演进路径做出的自然推演。现在业界其实也逐渐达成了某种共识。

至于未来会怎样？我觉得会碰到一个问题是：这些方向够不够？大语言模型的推理能力在一些难题上已经展现出很高水平，但它仍然会在一些非常简单的问题上出错。比如 GPT-5 发布时，有人问它“美国50个州里哪些州名字里包含字母 R”，结果它没答对。

另外，它在最困难的数学推理上，还是和顶尖人类有差距。比如说，今年国际奥林匹克数学竞赛一共有六道题，AI 能解出前五题，但第六题没有任何模型能做出来。IMO 美国教练埃文还专门点评过，说前五题算相对容易，最难的是第六题。所以在深度推理上，AI 还有很长的路要走。

你刚才问有没有让我们觉得意外的实验结果？其实让我感触最深的是：研究智能突破时，一定要避免方法过于精巧。太过精巧的话，容易导致模型出现 reward hacking，或者过拟合。真正有效的，往往是足够简单、具备通用性和泛化性。复杂的规则通常走不远。

这点其实和 Richard Sutton 提出的 The Bitter Lesson 是呼应的：过去几十年里，精巧的算法是没有强大的算力有效果的。

##### **主持人**

听起来，你们的研究既关注大方向上的规律，又非常强调方法的朴素性和普适性。那么，关于现在推理能力发展的瓶颈。从研究角度看，目前推理的主要难点在哪里？是数据不足、算法局限，还是硬件资源的限制呢？

##### **我**

是的，我们现在看到的情况是，过去几年里，智能的发展主要遵循“scaling law”，也就是算力、数据、模型规模，还有时间。这也是我们前面回顾过的一条智能进步路线。

但问题在于，这几个维度现在都进入了瓶颈。什么意思呢？就是虽然智能能随着scaling law进步，但大家发现scaling law本身是log-scale的。也就是说，你需要投入十倍的资源，才能换来一点点提升。资源消耗是指数级增长，但智能的提升往往只有线性增长。很多研究都已经显示出了这种情况。所以我们觉得，要想往前走得远，就必须找到scaling law之外的新范式。

我们希望从人类提高智能的方式中提取一些新猜想。我们的猜想从一个非常聪明、受过良好训练的博士生开始。他符合scaling law的四个维度，他的大脑容量很强大，他博学，受过数十年严谨训练，学过大量知识，很擅长思考。

但为什么他还要继续读博士、跟随导师训练？就是因为光有知识和能力还不够，他需要导师的引导。导师通常不会只是灌输知识，而是通过不断布置任务、提出问题来帮助学生训练思维。我们就在想，能不能模仿这种模式来训练大语言模型，让它也能突破scaling law瓶颈。

所以我们在实验中尝试过这种“引导式”的方法。面对一个复杂的问题时，我们不要求模型直接给出答案，而是引导它先做计划，把复杂问题分解成步骤，再对每个步骤细化并逐一实现和验证。

这是一个非常有意思的实验。结果让我们很惊讶，就是用一个这样简单的一个个约束性的引导性的问题，模型在数学推理上面的能力是出现了飞跃。我们在今年六月做的这个实验，发现它能在没有额外训练的情况下，达到当时顶级模型的水平。这让我们很有信心，相信这种方式可能帮助大模型突破当前的瓶颈。

大语言模型现在之所以会犯一些很糟糕甚至很愚蠢的错误，或者无法解决复杂的问题，原因可能是一样的：它缺乏把问题抽象出来的能力，也就是泛化能力还不够强。它可能见过很多例子，但却没办法从例子里总结出更高阶的规律。所以我们应该通过这种引导方式，把它潜在的抽象和规划能力激发出来。

另一个可能出现突破的方向，就是我们认为模型在处理复杂问题时，需要额外的机制辅助。我把它称为“长期记忆”。就像大脑依靠海马体储存长期记忆一样，大语言模型也需要长期记忆。现在它们已经有了“上下文窗口”，很多团队，包括我们自己，都在努力把窗口做得更大，因为窗口越大，能利用的信息就越多，思考也能更久。

但我们发现，像注意力机制这样的设计并不是完美的，它可能带来很多噪音。噪音在处理复杂问题时会成为致命弱点。所以我们设想，如果能把上下文窗口中的信息做处理、去噪，然后把关键的信息存储到长期记忆里，不仅能解决当前问题，还能对未来的问题提供帮助。

所以我觉得，长期记忆机制可能是提升大语言模型推理能力的另一个重要机遇。

我们将上面几种思路总结成为一种新范式，称之为agentic reasoning：基于智能体的AI推理。

##### **主持人**

你们在做推理研究时会考虑和现实任务结合，比如说代码的生成，复杂问题等等。你怎么看推理在整个AI系统里以及这些具体应用中的价值？

##### **我**

这是个好问题。我觉得推理能力其实就是智能能力突出体现的一个方向。Satya 最近有句话我很认同：就是说AI 能力再高，你最终还是要在GDP的增长中体现出来。换句话说，一个人的推理能力再高，也要在具体的生产活动中得到体现，那才是有用的对吧？ 

很明显推理是一个很核心的能力，在绝大多数的生产活动中都需要推理。你很难想象有多少工作是不需要推理的，尤其是面对复杂问题的时候。所以推理绝对是在所有的应用中是有非常大的一个价值。

##### **主持人**

我们接着聊下一个比较热的话题，就是“通用推理能力”。它意味着模型在跨任务、跨领域时都能灵活地进行逻辑推导。在你看来，这个目标离我们还有多远？

##### **我**

其实这个问题和上一个问题是相关的。把推理能力放到不同场景去用，自然就要所谓的通用推理能力。我觉得通用推理的基础还是推理，也就是说推理必须是对的，就得有逻辑。这也是为什么我们认为从数学角度训练AI的数学推理能力是非常本质的。如果它能在数学上能做到很严谨，那么它就更有可能把这种能力泛化到别的领域。就像数学系的学生去做别的事情，通常也能相对适应，但仍需要一些额外的训练。

##### **主持人**

嗯，听上去数学像是一种“通用底座”。那是不是说数学一强就万事大吉了？

##### **我**

这并不意味着数学推理能力很高了以后，它能自动在所有领域都达到很强的通用推理能力。这还牵涉到大量跨领域的知识获取问题。很多领域有自己的“行规”，有些甚至是私有的，比如公司的内部商业逻辑，这类信息通常不公开。在没有这些数据的情况下，怎么把一个有很强的通用推理能力的AI用好？这仍然有很多研究问题需要解决。

##### **主持人**

明白了。那我们换个角度聊聊。推理听起来挺“高冷”的，但其实和人类思维有不少联系。你觉得在人类的这些机制里，比如反思（reflection）、因果（causality）、常识（common sense），哪些最值得在推理中借鉴和实现？

##### **我**

这是个好问题。我觉得推理是个很宽泛的概念。我们自己在具体的实验中发现，如果要有很强的推理能力，就需要很多人类思维里的高阶思考能力。比如反思，这是目前观察到的一个重要的机制。

另外，创造性在推理中也很重要。实际上在越困难的问题中，创造性越重要。因为越困难就代表着常规的思路、常识可能已经很难解决了。这时候就需要“跳出框架”的思维，也就是创造性。还有刚才提到的好的抽象能力，这个推理才能有足够的泛化能力。以及，在处理复杂问题时，需要好的规划能力。所以几乎所有人类思维中的核心机制，在推理中都有所体现。

如果缺乏常识，就容易犯很低级的错误，这其实就是常识不足的体现。至于因果，这是一个更复杂的问题。我们能不能真的从看似相关的现象里推导出因果，这在学术界一直有争议。

从传统统计学派到其他学派，各自观点都不一样。大语言模型就是这种争议的一个矛盾体现：它好像在某些时候能表现出因果推理的能力，但严格来说它还是基于相关性。这也是为什么有人认为它只是“模拟推理”，而不是在真正理解因果。

##### **主持人**

对，它不一定是有因果关系，只不过是这两个有关联，对吧？

##### **我**

对，普遍的共识是，关联和因果在本质上是不一样的东西。所以你不可能仅仅通过观察关联，就真正理解这个推理。如果真是这样，那 AI 可能永远没有能力理解所谓的因果关系。

我的想法是，即使它不理解，但依然能解决非常复杂的问题，那对我们来说又有什么区别呢？从实用主义的角度来看，下面这个问题值得思考：你究竟要关心它是不是理解了，还是更关心它能不能帮你解决现实问题？这就是我的一些看法。

##### **主持人**

嗯，很有意思。那最后一个问题，咱们稍微展望一下未来。如果把时间加速到三到五年以后，你觉得推理技术最可能带来的质变会体现在哪些方面？

##### **我**

说实话，我也没法准确预测，只能猜测一下。现在的大语言模型表现出两种看似矛盾的现象。一方面，它的推理能力已经很强，强到普通人很难判断。在某些专业领域，它的表现已经超过大多数人。所以很难再去分辨它的能力是不是还在继续提升。但实际上，它确实在提高。

但另一方面，它依然会在一些很简单的地方犯错，而这些低级错误是普通人最容易感知的。我猜想三到五年后，模型可能还会犯这种错误，但概率会越来越低。但因为用户规模太大，这些错误依然会被放大。与此同时，它在一些专业领域的推理能力会进一步提升，可能沿着我们刚才提到的技术路径发展。

我相信未来三到五年，推理技术一定会更深入地进入现实生产环节，切实提高生产力。这几乎是一个高概率的趋势。

#### **结束语：**

好的，今天的对话真的是信息量很大。从推理的基本概念，到速度和准确性的平衡，再到符号主义和连接主义的结合，还有未来可能突破的方向，杨博士都分享了很多深入的思考。

我们也期待看到三到五年后，推理技术能更深刻地改变我们的工作和生活。

感谢杨凡博士带来的分享，也感谢大家的收听，希望我们的节目能帮你更好地理解AI技术。如果你喜欢这期节目，欢迎订阅和分享《AI Next》。我们会在接下来的节目里，继续带你探索人工智能的前沿。下期再见！
