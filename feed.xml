<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://fanyangcs.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://fanyangcs.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-18T12:18:02+00:00</updated><id>https://fanyangcs.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">A podcast on AI reasoning</title><link href="https://fanyangcs.github.io/blog/2025/OnAIReasoning/" rel="alternate" type="text/html" title="A podcast on AI reasoning"/><published>2025-12-05T12:00:00+00:00</published><updated>2025-12-05T12:00:00+00:00</updated><id>https://fanyangcs.github.io/blog/2025/OnAIReasoning</id><content type="html" xml:base="https://fanyangcs.github.io/blog/2025/OnAIReasoning/"><![CDATA[<p>Recently, I was on a <a href="https://fanyangcs.github.io/news/AIReasoningPodcast/">podcast</a> discussing AI reasoning. Below is the transcript of the podcast. It is in Chinese. I will see if I get a chance to have an English version.</p> <hr/> <h4 id="背景"><strong>背景：</strong></h4> <p><em>一提到“推理”，很多人第一反应可能是侦探小说里的福尔摩斯：通过蛛丝马迹一步步找出真相。人类的推理确实是这样，我们会依靠逻辑、常识和经验，从已知条件出发，逐步得出结论。</em></p> <p><em>那 AI 的推理呢？它当然不会去破案，但它要面对的，其实是类似的过程：从已有信息里，分析出合理的结果，甚至能够延伸出新的可能性。比如，你对一个智能助手说：“今天下雨了，我要出门”，它如果只回答“带伞”，这其实还不算推理；但如果它能继续想到“可能要穿防水外套”，“雨天路滑可以考虑公共交通”，这就是在做更接近人类的推理。</em></p> <p><em>但是，AI的推理能力也不是一开始就这么强的。在AI发展的历史中，我们通常把AI推理分为两个主要阵营：</em></p> <p><em>第一个是“符号推理”（Symbolic Reasoning）。这是比较传统的方法。你可以想象一下，它就是给AI设定了一堆严谨的逻辑规则，比如“如果 A 成立，并且 B 成立，那么 C 就成立”。它的优点是严谨、可解释，你可以清楚地知道AI是怎么得出结论的。但它的缺点也很明显：现实世界太复杂了，很难穷尽所有规则，而且面对不确定的、模糊的问题时，它就束手无策了。</em></p> <p><em>第二个是“非符号推理”（Non-symbolic Reasoning），这主要基于数据进行学习，通过神经网络发现数据中的隐藏模式。像我们现在经常听到的大语言模型，它们的推理能力就属于这一类。它不像符号推理那样有一条条清晰的逻辑链，更像是一种“直觉”。它在海量数据中看到了无数次“下雨”和“带伞”的关联，所以当你说“下雨”时，它就能“直觉”地给出“带伞”这个答案。它的优点是擅长处理复杂、开放的问题，但有时也会犯一些低级错误，甚至给出看似合理但实际上是错误的回答，也就是我们常说的“幻觉”。</em></p> <p><em>这两种方法各有优势和短板。而现在最前沿的研究，就是如何把它们结合起来，让AI既能有强大的“直觉”，又能进行严密的“逻辑”思考。</em></p> <p><em>带着这个背景，我们就更容易理解后面要聊的内容了。好了，话不多说，让我们进入今天的正题吧！</em></p> <h4 id="对话环节"><strong>对话环节：</strong></h4> <h5 id="主持人"><strong>主持人</strong></h5> <p>杨凡博士你好，现在我们先从一个基础的问题聊起。很多听众对“推理”的理解可能还停留在人类的逻辑推理层面，比如福尔摩斯那样的逻辑推理。那么在人工智能的语境下，我们说的“AI推理”和传统意义上的逻辑推理，到底有什么本质区别呢？能不能理解为，AI推理更多是基于大数据的概率性归纳？</p> <h6 id="我"><strong>我</strong></h6> <p>这个问题非常有意思。其实从某种意义上来说，AI推理和逻辑推理就像人工智能和人类智能的区别一样。人工智能研究的目标，就是希望做到接近人类智能的水平。而推理能力正是智能最本质的体现之一，所以这也是我们对智能特别感兴趣的一个原因。</p> <p>根据维基百科的介绍，推理就是从大量信息中得出结论的能力。如果得出的结论是令人信服的，那它就需要一定的逻辑。换句话说，为了保证推理的正确性，就需要遵循一些比较严谨的过程，这就是逻辑。从抽象层面上说，它可以用数学来表达。</p> <p>很多人会觉得大语言模型的推理是基于概率统计的大数据归纳。这牵涉到统计方面的一个本质问题：从大数据中归纳出来的现象，究竟意味着什么？是纯粹的相关性，还是因果性？如果只是相关性，那AI推理就不应该有因果性，可推理本身就是基于因果性的。</p> <p>如果AI 是基于大数据的归纳，它只是浮现出相关性，那么就不应该展现出强大的推理能力。现在的AI推理确实经常犯错，但它在过去两年的发展中，推理正确性也越来越高。而且以人类的知识来检验的话，它这中间展现出的因果性是相当惊人的。为什么会展现出这样的一种特性，以及这种特性能走多远？</p> <p>现在学界对这个问题并没有统一答案。我个人更倾向认为，在基于概率的模型上，我们仍然有机会把推理做到一个很高的水准，甚至达到实用层面。比如说，在初高中层面的逻辑和数学题上，大模型如果展现出相当可靠的推理能力，那么就有可能在实际应用里发挥价值。</p> <p>至于AI推理和传统意义上的人类推理的区别，我觉得目前还没有明确答案。就像人工智能和人类智能之间的差异一样，我们只知道人脑的生物学结构和大模型的构造完全不一样。但这并不意味着它们不能共享某些特性。</p> <p>我更愿意把智能看成是一件很“纯粹的事”， 智能就是智能，你可以用方法A来实现智能，也可以用方法B来实现。人类和大模型就是实现智能的两种不同方式，其中可能存在相关性。这是我现在的一个看法。</p> <h5 id="主持人-1"><strong>主持人</strong></h5> <p>你提到推理的正确性，这让我想到另一个问题。很多人觉得“推理就是让大模型更聪明”。可在学术界和工业界，大家又特别关注计算速度。那么 ，你怎么看推理速度和智能之间的关系？</p> <h5 id="我-1"><strong>我</strong></h5> <p>我觉得只有正确的推理才能算聪明，尤其是面对复杂问题的时候，能得出正确答案才是真正的智能。人和 AI 都会在速度和质量之间做平衡。对于一些问题，我们都希望得到对的答案。但想得快却经常出错，其实没有意义。大语言模型也是一样，光快不够，必须尽量保证推理正确。</p> <p>当然，速度在推理中也非常重要。很多复杂问题可能需要成千上万步的推理步骤，如果每一步都很慢，那最终问题也解不出来。而且人也不会等这么长时间，大多数情况下，我们还是希望答案尽快出来。</p> <p>另外，如果在一些相对简单的问题上都思考得很慢，那别人也很难相信你在复杂问题上即使坚持长时间能给出结果。我们通常觉得一个人聪明表现为思考非常快，当然回答也是对的。</p> <p>所以从智能的角度来说，速度和质量之间确实有一个微妙的平衡，不能做极端的选择。</p> <h5 id="主持人-2"><strong>主持人</strong></h5> <p>非常清晰的观点。所以在推理能力上，既要准确，也要兼顾速度，两者确实需要平衡。那接下来聊一聊你团队的研究方向。你的团队最近在探索新的推理方式，能不能给我们介绍一下研究内容？这些方法和传统的深度学习推理相比，最大的突破点在哪里？在实验过程中有没有什么让你们出乎意料的发现？</p> <h5 id="我-2"><strong>我</strong></h5> <p>我们现在在探讨的一个推理范式叫做 neural symbolic reasoning。Neural 就是神经，symbolic 就是符号主义的推导。其实这个范式并不算新，人工智能早期就有符号主义和连接主义的争论。在很长一段时间里，符号主义占优势，因为连接主义没有展现出让人兴奋的结果。某种意义上，今天的计算科学很多知识都还是属于符号主义的范畴。</p> <p>随着大语言模型的兴起，连接主义在一些传统计算机难以解决的问题上展现出惊人的实力，比如语音识别、图像识别和自然语言理解等等。这是传统计算机做不到的。于是现在大家普遍认为，连接主义，也就是大语言模型，可能是走向 AI最重要的技术路径。</p> <p>在这样的范式下，研究社区都在追求所谓的 scaling law，就是追求模型规模、训练数据规模和计算规模，把这三者进一步扩展，智能表现就会不断提升。去年下半年，大家又注意到了时间因素，我称之为scaling law的第四维度。这被称为 test-time scaling，也就是说，模型思考的时间越长，它的智能水平往往越高。</p> <p>我们团队在一两年前就开始关注这种推理范式。当时有两个很朴素的想法。第一，scaling law 本身很符合人类直觉，比如人的大脑容量比其他动物大，神经元数量更多，这完全符合scaling law，所以我们的智能表现就比别的更好了，对吧？</p> <p>第二个维度是数据，然后就是计算力。你要思考的时候，你需要的这个脑力是很高的。这个脑力就相当于计算力。我们那个时候就觉得人类进一步提高，那不就是要思考的久一点吗？所以，当时这个范式还没有出现的时候，我们就认为我们肯定是要在脑子中不断思考。我们当时把这种范式称之为 self-play，也就是在脑中不断演练思考。我们去年的成果第一次用公开的方法告诉大家self-play，或者说所谓的test time scaling是怎么样提升智能的。这也是我们觉得探索智能进步的路径可以参考人类的一个原因。</p> <p>我们还思考neural symbolic reasoning。这是什么意思呢？我们认为从符号主义和连接主义的角度来讲，他们可能是一种互补关系。那时我们看到大语言模型推理肯定有很多很多问题，对吧？毕竟两年前大语言模型推理水平还停留在小学阶段，现在已经可以把奥林匹克的数学竞赛的金牌都给赢下来。我们当时也并不是认为大语言模型做不到，只是它也像人一样需要一个“脚手架”。 也就是用符号化的工具来帮助大语言模型在推理的时候做一种验证，所谓的verification，有了这个verification，那么就能极大地提高推理的可靠性。</p> <p>人类也会犯错，但我们可以通过复核、老师批改、甚至借助工具来验证，对吧？比如人类经常用的一些工具，比如数学的工具，像计算器。对 AI 来说，我们大量使用了自动定理证明来做验证。我们目前来讲的话，看到的这个范式也是起到了一个作用。</p> <p>事实上，研究社区里现在也在讨论类似的方向，虽然名字可能不同，比如有人叫 self-verifiable learning，但本质是一样的。推理在数学领域是相对比较容易被验证的，所以你会看到现在前沿的大模型都在打数学竞赛、编程比赛。这也是因为这些领域可以提供清晰的验证标准。</p> <p>所以我认为我们过去探索的这些方向算不上新的范式，而是沿着智能演进路径做出的自然推演。现在业界其实也逐渐达成了某种共识。</p> <p>至于未来会怎样？我觉得会碰到一个问题是：这些方向够不够？大语言模型的推理能力在一些难题上已经展现出很高水平，但它仍然会在一些非常简单的问题上出错。比如 GPT-5 发布时，有人问它“美国50个州里哪些州名字里包含字母 R”，结果它没答对。</p> <p>另外，它在最困难的数学推理上，还是和顶尖人类有差距。比如说，今年国际奥林匹克数学竞赛一共有六道题，AI 能解出前五题，但第六题没有任何模型能做出来。IMO 美国教练埃文还专门点评过，说前五题算相对容易，最难的是第六题。所以在深度推理上，AI 还有很长的路要走。</p> <p>你刚才问有没有让我们觉得意外的实验结果？其实让我感触最深的是：研究智能突破时，一定要避免方法过于精巧。太过精巧的话，容易导致模型出现 reward hacking，或者过拟合。真正有效的，往往是足够简单、具备通用性和泛化性。复杂的规则通常走不远。</p> <p>这点其实和 Richard Sutton 提出的 The Bitter Lesson 是呼应的：过去几十年里，精巧的算法是没有强大的算力有效果的。</p> <h5 id="主持人-3"><strong>主持人</strong></h5> <p>听起来，你们的研究既关注大方向上的规律，又非常强调方法的朴素性和普适性。那么，关于现在推理能力发展的瓶颈。从研究角度看，目前推理的主要难点在哪里？是数据不足、算法局限，还是硬件资源的限制呢？</p> <h5 id="我-3"><strong>我</strong></h5> <p>是的，我们现在看到的情况是，过去几年里，智能的发展主要遵循“scaling law”，也就是算力、数据、模型规模，还有时间。这也是我们前面回顾过的一条智能进步路线。</p> <p>但问题在于，这几个维度现在都进入了瓶颈。什么意思呢？就是虽然智能能随着scaling law进步，但大家发现scaling law本身是log-scale的。也就是说，你需要投入十倍的资源，才能换来一点点提升。资源消耗是指数级增长，但智能的提升往往只有线性增长。很多研究都已经显示出了这种情况。所以我们觉得，要想往前走得远，就必须找到scaling law之外的新范式。</p> <p>我们希望从人类提高智能的方式中提取一些新猜想。我们的猜想从一个非常聪明、受过良好训练的博士生开始。他符合scaling law的四个维度，他的大脑容量很强大，他博学，受过数十年严谨训练，学过大量知识，很擅长思考。</p> <p>但为什么他还要继续读博士、跟随导师训练？就是因为光有知识和能力还不够，他需要导师的引导。导师通常不会只是灌输知识，而是通过不断布置任务、提出问题来帮助学生训练思维。我们就在想，能不能模仿这种模式来训练大语言模型，让它也能突破scaling law瓶颈。</p> <p>所以我们在实验中尝试过这种“引导式”的方法。面对一个复杂的问题时，我们不要求模型直接给出答案，而是引导它先做计划，把复杂问题分解成步骤，再对每个步骤细化并逐一实现和验证。</p> <p>这是一个非常有意思的实验。结果让我们很惊讶，就是用一个这样简单的一个个约束性的引导性的问题，模型在数学推理上面的能力是出现了飞跃。我们在今年六月做的这个实验，发现它能在没有额外训练的情况下，达到当时顶级模型的水平。这让我们很有信心，相信这种方式可能帮助大模型突破当前的瓶颈。</p> <p>大语言模型现在之所以会犯一些很糟糕甚至很愚蠢的错误，或者无法解决复杂的问题，原因可能是一样的：它缺乏把问题抽象出来的能力，也就是泛化能力还不够强。它可能见过很多例子，但却没办法从例子里总结出更高阶的规律。所以我们应该通过这种引导方式，把它潜在的抽象和规划能力激发出来。</p> <p>另一个可能出现突破的方向，就是我们认为模型在处理复杂问题时，需要额外的机制辅助。我把它称为“长期记忆”。就像大脑依靠海马体储存长期记忆一样，大语言模型也需要长期记忆。现在它们已经有了“上下文窗口”，很多团队，包括我们自己，都在努力把窗口做得更大，因为窗口越大，能利用的信息就越多，思考也能更久。</p> <p>但我们发现，像注意力机制这样的设计并不是完美的，它可能带来很多噪音。噪音在处理复杂问题时会成为致命弱点。所以我们设想，如果能把上下文窗口中的信息做处理、去噪，然后把关键的信息存储到长期记忆里，不仅能解决当前问题，还能对未来的问题提供帮助。</p> <p>所以我觉得，长期记忆机制可能是提升大语言模型推理能力的另一个重要机遇。</p> <p>我们将上面几种思路总结成为一种新范式，称之为agentic reasoning：基于智能体的AI推理。</p> <h5 id="主持人-4"><strong>主持人</strong></h5> <p>你们在做推理研究时会考虑和现实任务结合，比如说代码的生成，复杂问题等等。你怎么看推理在整个AI系统里以及这些具体应用中的价值？</p> <h5 id="我-4"><strong>我</strong></h5> <p>这是个好问题。我觉得推理能力其实就是智能能力突出体现的一个方向。Satya 最近有句话我很认同：就是说AI 能力再高，你最终还是要在GDP的增长中体现出来。换句话说，一个人的推理能力再高，也要在具体的生产活动中得到体现，那才是有用的对吧？</p> <p>很明显推理是一个很核心的能力，在绝大多数的生产活动中都需要推理。你很难想象有多少工作是不需要推理的，尤其是面对复杂问题的时候。所以推理绝对是在所有的应用中是有非常大的一个价值。</p> <h5 id="主持人-5"><strong>主持人</strong></h5> <p>我们接着聊下一个比较热的话题，就是“通用推理能力”。它意味着模型在跨任务、跨领域时都能灵活地进行逻辑推导。在你看来，这个目标离我们还有多远？</p> <h5 id="我-5"><strong>我</strong></h5> <p>其实这个问题和上一个问题是相关的。把推理能力放到不同场景去用，自然就要所谓的通用推理能力。我觉得通用推理的基础还是推理，也就是说推理必须是对的，就得有逻辑。这也是为什么我们认为从数学角度训练AI的数学推理能力是非常本质的。如果它能在数学上能做到很严谨，那么它就更有可能把这种能力泛化到别的领域。就像数学系的学生去做别的事情，通常也能相对适应，但仍需要一些额外的训练。</p> <h5 id="主持人-6"><strong>主持人</strong></h5> <p>嗯，听上去数学像是一种“通用底座”。那是不是说数学一强就万事大吉了？</p> <h5 id="我-6"><strong>我</strong></h5> <p>这并不意味着数学推理能力很高了以后，它能自动在所有领域都达到很强的通用推理能力。这还牵涉到大量跨领域的知识获取问题。很多领域有自己的“行规”，有些甚至是私有的，比如公司的内部商业逻辑，这类信息通常不公开。在没有这些数据的情况下，怎么把一个有很强的通用推理能力的AI用好？这仍然有很多研究问题需要解决。</p> <h5 id="主持人-7"><strong>主持人</strong></h5> <p>明白了。那我们换个角度聊聊。推理听起来挺“高冷”的，但其实和人类思维有不少联系。你觉得在人类的这些机制里，比如反思（reflection）、因果（causality）、常识（common sense），哪些最值得在推理中借鉴和实现？</p> <h5 id="我-7"><strong>我</strong></h5> <p>这是个好问题。我觉得推理是个很宽泛的概念。我们自己在具体的实验中发现，如果要有很强的推理能力，就需要很多人类思维里的高阶思考能力。比如反思，这是目前观察到的一个重要的机制。</p> <p>另外，创造性在推理中也很重要。实际上在越困难的问题中，创造性越重要。因为越困难就代表着常规的思路、常识可能已经很难解决了。这时候就需要“跳出框架”的思维，也就是创造性。还有刚才提到的好的抽象能力，这个推理才能有足够的泛化能力。以及，在处理复杂问题时，需要好的规划能力。所以几乎所有人类思维中的核心机制，在推理中都有所体现。</p> <p>如果缺乏常识，就容易犯很低级的错误，这其实就是常识不足的体现。至于因果，这是一个更复杂的问题。我们能不能真的从看似相关的现象里推导出因果，这在学术界一直有争议。</p> <p>从传统统计学派到其他学派，各自观点都不一样。大语言模型就是这种争议的一个矛盾体现：它好像在某些时候能表现出因果推理的能力，但严格来说它还是基于相关性。这也是为什么有人认为它只是“模拟推理”，而不是在真正理解因果。</p> <h5 id="主持人-8"><strong>主持人</strong></h5> <p>对，它不一定是有因果关系，只不过是这两个有关联，对吧？</p> <h5 id="我-8"><strong>我</strong></h5> <p>对，普遍的共识是，关联和因果在本质上是不一样的东西。所以你不可能仅仅通过观察关联，就真正理解这个推理。如果真是这样，那 AI 可能永远没有能力理解所谓的因果关系。</p> <p>我的想法是，即使它不理解，但依然能解决非常复杂的问题，那对我们来说又有什么区别呢？从实用主义的角度来看，下面这个问题值得思考：你究竟要关心它是不是理解了，还是更关心它能不能帮你解决现实问题？这就是我的一些看法。</p> <h5 id="主持人-9"><strong>主持人</strong></h5> <p>嗯，很有意思。那最后一个问题，咱们稍微展望一下未来。如果把时间加速到三到五年以后，你觉得推理技术最可能带来的质变会体现在哪些方面？</p> <h5 id="我-9"><strong>我</strong></h5> <p>说实话，我也没法准确预测，只能猜测一下。现在的大语言模型表现出两种看似矛盾的现象。一方面，它的推理能力已经很强，强到普通人很难判断。在某些专业领域，它的表现已经超过大多数人。所以很难再去分辨它的能力是不是还在继续提升。但实际上，它确实在提高。</p> <p>但另一方面，它依然会在一些很简单的地方犯错，而这些低级错误是普通人最容易感知的。我猜想三到五年后，模型可能还会犯这种错误，但概率会越来越低。但因为用户规模太大，这些错误依然会被放大。与此同时，它在一些专业领域的推理能力会进一步提升，可能沿着我们刚才提到的技术路径发展。</p> <p>我相信未来三到五年，推理技术一定会更深入地进入现实生产环节，切实提高生产力。这几乎是一个高概率的趋势。</p> <h4 id="结束语"><strong>结束语：</strong></h4> <p>好的，今天的对话真的是信息量很大。从推理的基本概念，到速度和准确性的平衡，再到符号主义和连接主义的结合，还有未来可能突破的方向，杨博士都分享了很多深入的思考。</p> <p>我们也期待看到三到五年后，推理技术能更深刻地改变我们的工作和生活。</p> <p>感谢杨凡博士带来的分享，也感谢大家的收听，希望我们的节目能帮你更好地理解AI技术。如果你喜欢这期节目，欢迎订阅和分享《AI Next》。我们会在接下来的节目里，继续带你探索人工智能的前沿。下期再见！</p>]]></content><author><name></name></author><summary type="html"><![CDATA[How to fill the seemingly insurmountable gap between “correlation” and “causality”?]]></summary></entry><entry><title type="html">The “dollar per token” business</title><link href="https://fanyangcs.github.io/blog/2025/dollartokenbusiness/" rel="alternate" type="text/html" title="The “dollar per token” business"/><published>2025-10-08T12:00:00+00:00</published><updated>2025-10-08T12:00:00+00:00</updated><id>https://fanyangcs.github.io/blog/2025/dollartokenbusiness</id><content type="html" xml:base="https://fanyangcs.github.io/blog/2025/dollartokenbusiness/"><![CDATA[<p>One of the interesting business models in AI right now is what I like to call the “dollar per token” business. It’s a bit like how cloud companies charge you per I/O request, except here, you’re paying for every <em>token</em>, a chunk of text your AI sends or receives.</p> <p>The more tokens you use, the happier the seller.</p> <p>This simple and intuitive model has a twist: if the same company that sells you tokens also builds the model, it suddenly has a quiet incentive to make the model talk more. Longer replies mean more tokens. More tokens mean more dollars.</p> <p>And that sets up a little tension between users (who want quick, to-the-point answers) and model providers (who wouldn’t mind a bit of extra verbosity).</p> <p>But where there’s tension, there’s opportunity.</p> <p>We might soon see a new kind of AI business emerge — one that charges per <em>task</em>, not per token.</p> <p>Imagine an AI help-desk service that bills you only when it actually solves your tech issue, not for every word it types along the way. That’s like how Walmart or Costco makes money by selling goods. Their incentive aligns with their customers, i.e., to keep the prices down. A task-based AI would think the same way:</p> <p><em>“How do I give you the answer in fewer tokens?”</em></p> <p>If that happens, the economics of AI could shift from charging for how much the model talks to charging for what the model gets done.</p> <p>Doesn’t that feel like a healthier direction for everyone?</p>]]></content><author><name></name></author><summary type="html"><![CDATA[where there's tension, there's opportunity; the economics of AI]]></summary></entry><entry><title type="html">To prospective interns</title><link href="https://fanyangcs.github.io/blog/2024/intern/" rel="alternate" type="text/html" title="To prospective interns"/><published>2024-08-01T12:00:00+00:00</published><updated>2024-08-01T12:00:00+00:00</updated><id>https://fanyangcs.github.io/blog/2024/intern</id><content type="html" xml:base="https://fanyangcs.github.io/blog/2024/intern/"><![CDATA[<p>If you consider yourself a good systems guy and looking for an internship at MSR-Asia, please drop me an email. I also recruit students taking gap year.</p> <p>If you are interested in systems research and don’t know where to begin with, you could start by reading the papers mentioned <a href="https://www.sigops.org/awards/hof/">here</a>.</p> <p>If you are looking for advice on your research, Richard Hamming’s talk on “<a href="/assets/pdf/YouAndYourResearch.pdf">You and Your Research</a>” is a must-read.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[a note to students curious about systems research]]></summary></entry><entry><title type="html">On annual performance review</title><link href="https://fanyangcs.github.io/blog/2024/PerformanceReview/" rel="alternate" type="text/html" title="On annual performance review"/><published>2024-07-01T12:00:00+00:00</published><updated>2024-07-01T12:00:00+00:00</updated><id>https://fanyangcs.github.io/blog/2024/PerformanceReview</id><content type="html" xml:base="https://fanyangcs.github.io/blog/2024/PerformanceReview/"><![CDATA[<p>I find this <a href="https://www.norvig.com/performance-review.html">annual performance review</a> amusing. It happened in ‘05. Not 2005, but 1905.</p> <p>In the year of 1905, the person being reviewed was given an overall rating of 3 out of 5, i.e., an average score. His manager commented that the person devoted his time to “publishing a series of outside papers” and “he has done reasonably well”.</p> <p>As a matter of fact, in 1905, the person published 4 papers. <a href="https://doi.org/10.1002/andp.19053220607">One</a> of them, “Über einen die Erzeugung und Verwandlung des Lichtes betreffenden heuristischen Gesichtspunkt” (or “On a heuristic viewpoint concerning the production and transformation of light” in English), eventually led to the Nobel Prize in Physics in <a href="https://www.nobelprize.org/prizes/physics/1921/summary/">1921</a>. And this wasn’t even his best result that year – the other being his <a href="https://doi.org/10.1002/andp.19053221004">special theory of relativity</a>.</p> <p>From his manager’s perspective, this was an average year for Albert Einstein. For Physics and Human Being, however, this was the “<a href="https://en.wikipedia.org/wiki/Annus_mirabilis_papers">annus mirabilis</a>” (miraculous year).</p>]]></content><author><name></name></author><summary type="html"><![CDATA[performance review is hard ...]]></summary></entry><entry><title type="html">How disruptive ideas were treated?</title><link href="https://fanyangcs.github.io/blog/2023/DisruptiveIdeas/" rel="alternate" type="text/html" title="How disruptive ideas were treated?"/><published>2023-07-02T12:00:00+00:00</published><updated>2023-07-02T12:00:00+00:00</updated><id>https://fanyangcs.github.io/blog/2023/DisruptiveIdeas</id><content type="html" xml:base="https://fanyangcs.github.io/blog/2023/DisruptiveIdeas/"><![CDATA[<p>As researchers, we are all encouraged to work on important problems, presumably disruptive ones.</p> <p>However, history tells us that disruptive ideas are often not welcome, at least in the beginning.</p> <p>And here is a perfect example: <a href="/assets/pdf/reject.pdf">reject letters</a> for papers that eventually led to ACM A.M. Turing Award.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[of course, no one likes disruptive ideas]]></summary></entry><entry><title type="html">My reading list</title><link href="https://fanyangcs.github.io/blog/2023/readinglist/" rel="alternate" type="text/html" title="My reading list"/><published>2023-04-23T12:00:00+00:00</published><updated>2023-04-23T12:00:00+00:00</updated><id>https://fanyangcs.github.io/blog/2023/readinglist</id><content type="html" xml:base="https://fanyangcs.github.io/blog/2023/readinglist/"><![CDATA[<p>以下是2023年世界读书日(4.23)我应微软亚洲研究院之邀提供的书单。</p> <hr/> <p>作为一位标准的理工男，我从小喜欢的书一直都是只有一类：科技类读物。记得小时候刚识字，每周最盼望的就是一本叫《少年科学》的32开期刊。每期《少年科学》里面的第一栏就是一部科幻短篇小说。时至今日，我已记不清那些短篇小说的内容。然而，小说读完后给我带来的对未来的憧憬、对技术进步的向往这种情绪让我至今难忘。可以说这一系列32开的小小读物让当时的我对未来要从事的行业从未怀疑过：当个科学家。目前看来，我的职业生涯和儿时的梦想偏差不算大。😊</p> <p>回首过去，阅读确实影响了我的人生。无论是科幻名著里的各种奇思妙想，还是科普读物中介绍的各种科学知识和概念，这些信息中所透露出的深刻思想让我坚信，科学探索及其衍生的技术发明是推动人类社会发展的最关键力量。</p> <p>借着“世界读书日”，和大家分享一些我读过的有趣的书。</p> <h5 id="基地系列-foundation-series艾萨克阿西莫夫isaac-asimov"><strong>《基地》系列</strong> (Foundation Series)，艾萨克.阿西莫夫(Isaac Asimov)</h5> <p>科幻小说爱好者很少不知道大名鼎鼎的《基地》系列。我初次读到《基地》还是研究生时期。那时候《基地》系列在国内还只能找到最早出版于1951年的第一本基地小说，其余几部连中文版都没有。经过我同学的帮助，我才辗转从国外拿到了其余的原版小说。在《基地》中，科幻大师阿西莫夫刻画了一个庞大的，正在陷入衰退的银河帝国。数学家哈里.谢顿(Hari Seldon)通过他自创的一门学科，心理史学(Psychohistory), 预测出银河帝国将陷入长达三万年的黑暗年代。在这期间，社会从文明陷入野蛮，科学技术被遗忘，人类重返蒙昧。 通过心理史学的分析，谢顿认为帝国的衰退虽然无法避免，但是通过建立一个“基地”，人类可以大大缩短衰退的时间，从三万年减少到一千年。而“基地“就是未来人类重返光明的种子。</p> <p>《基地》系列的有趣之处在于，在基地文中描述的情节中我常常可以找到和人类历史发展相似之处。比如，对清高、神圣的知识的绝对崇拜反而让人类陷入衰退和蒙昧；而通过世俗的贸易可以大大促进人类社会的交流，充满烟火气的商业需求可以刺激技术的发展，从而让社会重新进入发展轨道。在现代社会，人类作为一个整体对知识的遗忘是不可想象的事情。而《基地》却描写了人类重返蒙昧的一个切实可能的路径。这对当时还在求学的我产生了触动：对科学本身的追求，也是可能陷入误区的。</p> <p>《基地》的另外一个有趣之处就是对其虚构的“心理史学”的演绎。作为一门“新兴学科”，心理史学的核心意义在于，虽然单个人类个体的行为难以预测，人类社会作为整体却可以被数学模型描述，因此心理史学可以对社会整体的演变进行有统计意义的预测。当时看到这个词的时候，我心里产生了一种荒谬的感觉。第一次看到有人把“算命”讲得这么“严谨”。然而，人类社会发展不就是有规律的吗？事实上，经济学家现在不就是在用数学工具来解释，甚至预测人类社会的经济活动吗？</p> <p>正是这种大胆、看似荒诞、又似乎“有点道理”的演绎让《基地》系列将科幻小说的魅力发挥淋漓尽致，成为科幻小说迷中的经典之作。</p> <h5 id="我机器人系列-i-robot-艾萨克阿西莫夫"><strong>《我，机器人》系列</strong> (I, Robot), 艾萨克.阿西莫夫</h5> <p>《我，机器人》系列可能是阿西莫夫一系列作品中比《基地》影响力更大的一部科幻小说集。其著名的“机器人三定律”更是在科幻迷中脍炙人口。阿西莫夫在《机器人》系列中构造了精巧的情节来展示看似严谨的机器人三定律是如何破绽百出，甚至完全服从三定律的机器人仍然可能造成伤害。</p> <p>在我看来，《机器人》系列中最有趣的地方就是阿西莫夫创造了机器人心理学（Robopsychology）（像不像Psychohistory?），并设置了机器人心理学家这一职位。在小说中，机器人心理学家以机器人三定律为基础，解开了很多看似怪诞、令人困惑、难以解释的机器人行为。在我印象最深刻的短篇故事《风险》（Risk）中，机器人心理学家苏珊.卡尔文博士(Dr. Susan Kalvin)通过一系列巧妙的对话成功地让机器人在不违反三定律的情况下建造出能在“超空间”中穿梭的太空飞船，并用其安全地运送人类。</p> <p>在2023年再回顾这篇短文，不禁让我联想到现在的“提示词工程师”（Prompt Engineer）。这些工程师想出了很多提示词（Prompt），就是为了用这些提示词让人工智能大型神经网络模型输出想要的结果，比如让Midjourney画出满意的图片或让ChatGPT给出满意的回答。这些提示词工程师现在不就像苏珊卡尔文博士当年和机器人对话那样在和人工智能模型对话吗？这些提示词工程师是不是又可以被称为人工智能模型心理学家呢？</p> <p>令人惊叹的是，尽管阿西莫夫的这些机器人故事主要诞生于1940/50年代，他描写的机器人与人类间的复杂关系，还有对机器人技术安全性的思考甚至到现在对人工智能技术的发展都有借鉴意义。在自动驾驶等人工智能领域，人们经常用机器人三定律来类比当前人工智能技术在安全性方面所面临的困境。现在看来，这一科学幻想小说系列更像是八十多年前阿西莫夫对未来人类社会发展的伟大预言。</p> <h5 id="时间简史a-brief-history-of-time史蒂芬霍金stephen-hawking"><strong>《时间简史》</strong>（A brief history of time），史蒂芬.霍金（Stephen Hawking）</h5> <p>《时间简史》是理论物理学家霍金博士的科普名著，主要介绍了科学家对宇宙来龙去脉的解释和推测。在科技工作者这一群体中，没有听过这本书的人可能很少了。霍金在书里对宏观物理理论的发展，从大爆炸理论，到时空相对论，再到黑洞理论，做了精彩的介绍。在读到这本书的时候已经是我的大学时代，对于物理，对于宇宙， 我的了解已非当年懵懂少年。然而可以说这本书影响了我的人生。书中不但通俗易懂地介绍了看起来晦涩的物理理论，还阐述了物理学家在探寻这些理论的过程中对宇宙本源问题的终极思考。宇宙为什么是这样的？是必然的还是由看不见的手决定的？这种对本源问题的深入探索让我大开眼界，第一次从门口窥到了所谓“深入思考”大概是什么境界。</p> <p>在其他人眼中，《时间简史》可能是一本优秀的科普读物。而在我眼里，霍金通过这本书向我展示了科学研究的精神内核。</p>]]></content><author><name></name></author><summary type="html"><![CDATA[以下是2023年世界读书日(4.23)我应微软亚洲研究院之邀提供的书单。]]></summary></entry><entry><title type="html">关于研究的一些感悟</title><link href="https://fanyangcs.github.io/blog/2023/onresearch/" rel="alternate" type="text/html" title="关于研究的一些感悟"/><published>2023-04-11T12:00:00+00:00</published><updated>2023-04-11T12:00:00+00:00</updated><id>https://fanyangcs.github.io/blog/2023/onresearch</id><content type="html" xml:base="https://fanyangcs.github.io/blog/2023/onresearch/"><![CDATA[<p>以下文字是我应邀为微软亚洲研究院“树洞计划”关于研究方面的问题提供回答的完整版。 研究院公众号的<a href="https://www.microsoft.com/en-us/research/articles/ask-me-1/">版本</a>有所删节。 文中有关同事的一些轶事是基于我的记忆描写的，如有偏差请见谅。</p> <hr/> <h4 id="提问"><strong>提问：</strong></h4> <ul> <li> <p>在做研究时，对问题的思考总是浮在表面上，每次需要更深入的思考时，总无法触及问题的核心。如何才能让自己的思维不仅停留在问题表面？</p> </li> <li> <p>科研小白找 idea 真的好难，对领域并不那么熟悉，也不太知道想到了之后能不能 work，要反复想反复验证，一个 idea 一个 idea 地换，好累呀….</p> </li> <li> <p>找不到科研方向怎么办？</p> </li> <li> <p>在科研生活中，面对末知，找不到前进的方向时，如何制止自己的慌乱、思想抛锚与死磕（钻牛角尖），以让自己冷静理性？</p> </li> </ul> <h4 id="回答"><strong>回答：</strong></h4> <p>这些都是非常好的问题，非常值得探讨。我深切理解同学们在做研究时所面临的迷茫和焦虑。作为一位”过来人”，我对这些问题可以说是感同身受。 即使到现在，我还在不断思考和探索如何将研究做得更深，如何把握未来趋势。我认为，这些问题没有绝对正确且统一的答案。 不过，作为一名长期在系统领域工作的研究员，我确实积累了一些个人心得。借此机会，我想将这些经验分享出来，希望对大家有所启发。</p> <h5 id="正视困难接受挑战"><strong>正视困难，接受挑战</strong></h5> <p>严肃的研究是非常具有挑战性的。我还记得微软亚洲研究院前院长沈向洋有一次在闲谈中讨论过做研究的困难。 他大致的观点是：取得扎实的研究成果是一件很困难的事情，而且这并不会因为你以前做出过好结果，下一次就会变得容易。 听到这些话以后，我反而没有那么焦虑了。原来一流的研究员也觉得研究很困难啊！ 在自己的研究项目中，我也体会到：在面对一流的研究问题时，人人平等，问题本身并不会因为你是谁变得更容易。 有鉴于此，面对困难，我的建议是首先要坦然面对，接受做研究就是很困难这一现实。在科研过程中”面对未知，找不到前进方向“是常态，而找到突破点实际上是罕见的。 接受这一现实，在研究过程中我们的心态就会更加平和，避免浮躁。</p> <h5 id="积极思考厚积薄发"><strong>积极思考，厚积薄发</strong></h5> <p>在进行研究时，我们经常会遇到对新领域不熟悉，难以找到合适题目，或者老师给了题目也无法深入开展下去等问题。 到现在我和我的同事还有实习生们也会时常陷入这样的困境。但在实际工作中，我发现一些思维技巧可以帮助我们更容易地脱困而出。</p> <p>我经常运用的一种思考方法是追根溯源，多问为什么。当面临问题时，要问自己为什么会遇上这个问题？当想到某个方法时，可以问自己为什么会考虑这个方法？ 对某个方案，要问自己为什么它与众不同？为什么它可能行得通，又为什么可能行不通？持续追问，直至问题的答案可以归约为领域中某个公认的设计原则 （比如系统设计中的separation of concern，modularization, layering，minimization等原则），或者问题或答案可以被形式化，并用准确的数学语言加以描述。 通常到这个阶段，我们对这个问题得理解就比较深刻了。</p> <p>另一个思维技巧就是将问题抽象化（Abstraction）。我们要思考这个想法是否可以用更抽象的方式描述？它是不是代表着一类更普遍、更通用的问题？ 例如，在研究深度学习集群调度和深度学习编译器的时候，我和同事们常问自己，是否可以采用更通用的技术，而不仅仅局限于深度学习这一特定场景来解决问题。 最终，这种思考方式反而让我们找到了一些不得不用的专门的技术方案。</p> <p>还有一个方法就是从多个角度思考同一个问题。比如想证明一个问题的时，多问自己是否可以证伪这个问题（反证法）。 在考虑编译技术时，是否可以从作业调度的角度来解决。这些例子都是我在研究过程中实际遇到的情况。</p> <p>在思考过程中，我们常常会遇到瓶颈。这时，与其他同事交流往往会有帮助。在工作中，我发现在一个开放、放松的环境中和同事交流，有时仅仅向同事讲述自己正在进行的工作就能激发新灵感。 在交流过程中，关注对方的“非语言”反馈十分重要。由于同事间的关系，有些同事可能不会直言对某项工作的看法。但他们的肢体语言和表情通常会“说实话”。 此时，我们应多探究为何他们会有这样的反应。是因为没有听懂，不感兴趣，还是不喜欢、不赞成？这些信号对提高工作质量至关重要。</p> <p>最后，当思考陷入瓶颈时，可以考虑暂时放下问题，转换思维。在工作中，我和我的同事通常会同时考虑几个问题。一个问题解决不了就去思考其他问题。 我个人在某些问题上花了几年都没有进展，但有时在一个契机出现时突然就有了眉目。例如，自2018年起，我们就发现深度学习算子编译起来非常耗时，通常需要几个小时甚至更长时间。 那时我们在想能否重用某些编译结果，以节省编译时间？我们尝试了多种方法都没有进展，但这个想法在跨模型编译时反而显现出效果。 2020年，我们开发出的Retiarii（OSDI’20）系统采用了这一想法。两年后，我们在快速高效地编译深度学习算子上也取得了突破，做出了Roller（OSDI’22）这一系统。</p> <p>综上所述，保持积极的思考态度并灵活运用各种思维技巧对学术研究很有帮助。 通过追根溯源、问题抽象化、从多个角度审视问题、与同事进行有效交流以及在遇到瓶颈时转换思维等方法，我们可以让自己更深入地思考，真正沉淀下来，最终取得突破，实现厚积薄发。</p> <blockquote> <p>我相信，幸运之神会眷顾那些准备好的头脑。</p> </blockquote> <h5 id="保持激情勇担失败"><strong>保持激情，勇担失败</strong></h5> <p>在研究过程中，我们时常会遇到挫折和失败，这种时刻很容易让我们陷入患得患失的情绪。然而，持续的负面情绪不仅对身心健康有害，还会削弱我们的研究热情和创造力。因此，如何应对这种情绪至关重要。</p> <p>在面对挑战时，我个人的体会是在研究过程中要保持好奇心，专注于深入理解事物的本质，而非只关注简单的成败。实际上，在研究过程中，弄清楚某个现象背后的原因比仅展示结果更为重要。因此，当遭遇失败时，我们应该努力理解失败的原因。随着对问题的理解逐渐加深，我们就能够洞察到别人无法感知的见解（insight）。这种深刻的洞察力往往会引领我们发现新大陆，实现重要的突破。</p> <p>此外，保持研究的激情是让我们长期坚持、实现厚积薄发的重要原生动力。在这方面，现任微软亚洲研究院院长周礼栋博士就是个很好的例子。 2019年春天，周博士突然告诉我们最近火热的区块链技术让他对分布式共识技术有了些新想法并刚完成论文初稿。我听了很诧异，最近那么忙，他哪来的时间写论文？ 他回答说是趁着过春节时写的。周博士是分布式共识领域的专家，他的博士论文就是关于分布式共识的。如果不是对这一研究课题持久的兴趣，又是什么能让一位已经很有成就的研究员在假期工作呢？ 在周博士和他的合作伙伴们的持续努力下，该成果最终获得了2020年度系统领域顶级会议OSDI的Jay Lepreau最佳论文奖。如何找到并保持研究热情是一个非常个人化的问题，每个研究员可能都有自己独特的答案。 我们可以尝试从自己热爱的领域、感兴趣的问题出发，寻找激发研究热情的源泉，从而在研究道路上走得更远。</p> <h5 id="预测未来引领潮流"><strong>预测未来，引领潮流</strong></h5> <p>当然，仅靠激情是难以长久坚持的。归根结底，科研成果所带来的成就感才是我们继续前行的根本原因。对于新入行的同学，发表论文于顶级学术会议上是一种成就感。随着时间的推移，仅仅发表论文可能无法带来足够的成就感。研究员希望自己的成果能够产生广泛的影响，甚至改变某个领域对某个问题的看法。做出扎实、有影响力的研究工作是每个研究员孜孜不倦追求的目标。</p> <p>在这方面，我非常认同微软亚洲研究院“老一辈”研究员，微软杰出科学家郭百宁博士的观点。他一直鼓励我们勇敢地预测未来，努力成为引领潮流的研究员。他引导我们思考：未来5年，甚至10年内，你所从事的领域将会发展到什么程度？为了实现这样的发展，最重要的技术障碍是什么？你当前的研究是否在扫除这些障碍？妥善地回答这些问题有助于我们开展有影响力的工作。他还特别强调不要害怕预测错误。因为正如前文所述， “勇担失败”，对失败原因的深刻刨析通常会带来突破的契机。相反，若只是跟随别人的步伐，则很难做出开创性的工作。</p> <p>在勇敢地预测未来这方面，郭博士团队的高级研究员胡瀚是一个出色的榜样。在闲谈中，胡瀚告诉我，他相信“注意力”机制（Attention）在计算机视觉领域将发挥基础性的作用。基于这一预测，他和他的同事及实习生进行了长达数年的高强度研究。在当今研究节奏极快、项目通常需在数月甚至数周内见效的计算机视觉领域，这无疑是一个勇敢的举动。正是这种对未来的预测，让胡瀚和他的同事做出了广受关注的Swin Transformer这样一个项目。该成果不仅在学术界获得认可，斩获计算机视觉研究的最高荣誉马尔奖，而且在工业界，视觉领域的各个子任务常常将Swin Transformer作为标配之一。</p> <p><br/></p> <p>以上这些零散观点和经验仅仅是我个人的体会，难免有疏漏和谬误之处。但我希望它们能够对同学们产生一些帮助。要强调的是，每个研究员都有自己独特的经历和心得。探索科研之路，关键在于不断学习、实践和反思，逐渐形成自己的研究方法和思维模式。在科研生涯中，培养自己的创新能力、批判性思维和解决问题的技巧是一个长期、渐进的过程。与此同时，大家要逐渐学会与团队合作、与同行交流。在科研过程中，需要不断调整自己的心态，学会承受失败，保持研究热情和好奇心。科研是一条充满挑战的道路，但正是这种挑战才使得科学研究如此吸引人。在不断努力、尝试和取得进展的过程中，我感受到科研真正的乐趣在于探索未知、发现新大陆。对一个有挑战性的问题长期坚持，最终获得答案，这比简单的世俗认可更能带来满足感。我相信这也是广大科研工作者能够耐住寂寞，长期奋战在各自科研领域的根本原因。</p>]]></content><author><name></name></author><summary type="html"><![CDATA[幸运之神会眷顾那些准备好的头脑。]]></summary></entry></feed>